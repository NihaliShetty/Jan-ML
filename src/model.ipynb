{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingqia2/miniconda3/envs/qaprompts/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['COCO_DIR'] = 'coco2017/'\n",
    "os.environ['AOKVQA_DIR'] = '/usr1/data/mingqia2/aokvqa/'\n",
    "os.environ['HF_HOME'] = '/usr1/data/models_cache'\n",
    "\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n",
    "\n",
    "from load_aokvqa import load_aokvqa, get_coco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visual_clues(vqa_input):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vqa(sample, processor):\n",
    "    inputs = processor(\n",
    "        text=sample[\"question\"], images=sample[\"image_path\"], return_tensors=\"pt\", padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Process answer\n",
    "    labels = processor.tokenizer(sample[\"answer\"], return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    \n",
    "    # Process visual clues (textual metadata)\n",
    "    visual_clues = generate_visual_clues(sample)\n",
    "    metadata_inputs = processor.tokenizer(\n",
    "        visual_clues, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    return inputs, labels, metadata_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mc_loss = nn.CrossEntropyLoss()  # Loss for MC task\n",
    "        self.da_loss = nn.CrossEntropyLoss()  # Loss for DA task\n",
    "        # self.ce_loss = nn.CrossEntropyLoss() # overall loss  \n",
    "        # self.aux_loss = nn.MSELoss()  # Auxiliary loss for alignment\n",
    "    \n",
    "    def forward(self, mc_logits, da_logits, mc_labels, da_labels):\n",
    "        # Primary loss for VQA prediction\n",
    "        mc_loss = self.mc_loss(mc_logits, mc_labels)\n",
    "        da_loss = self.da_loss(da_logits.view(-1, da_logits.size(-1)), da_labels.view(-1))\n",
    "        total_loss = mc_loss + da_loss\n",
    "        return total_loss \n",
    "    \n",
    "        # Auxiliary loss to align metadata and question embeddings\n",
    "        # aux = self.aux_loss(metadata_embeddings, question_embeddings)\n",
    "        # return ce + 0.1 * aux  # Combine with weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset, processor, coco_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: List of samples with original question, answer, and visual clues.\n",
    "            processor: BLIP processor for text and image preprocessing.\n",
    "            image_dir: Path to the directory containing images.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.coco_dir = coco_dir\n",
    "        # self.split = split # train or val \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "         # Get image path and open image\n",
    "        image_path = get_coco_path(sample['split'], sample['image_id'], self.coco_dir)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Combine question with MC options\n",
    "        question = sample['question']\n",
    "        choices = sample['choices']\n",
    "        visual_clues = sample.get('visual_clues', '')  # List of question-answer pairs\n",
    "        \n",
    "        mc_question = f\"Question: {question} Visual Clues: {visual_clues} Choices: {', '.join(choices)}\"\n",
    "        da_question = f\"Question: {question} Visual Clues: {visual_clues}\"\n",
    "        \n",
    "        # Process image and augmented text\n",
    "        mc_encoding = self.processor(image, mc_question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        da_encoding = self.processor(image, da_question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Prepare answers \n",
    "        direct_answers = sample['direct_answers']\n",
    "        correct_choice_idx = sample['correct_choice_idx'] # index of the correct MC choice\n",
    "        \n",
    "        da_text = \" | \".join(direct_answers)\n",
    "        da_labels = self.processor.tokenizer.encode(\n",
    "            da_text, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        mc_label = torch.tensor(correct_choice_idx, dtype=torch.long)\n",
    "        \n",
    "        encoding = {\n",
    "            \"pixel_values\": mc_encoding[\"pixel_values\"],  # Same image for both versions\n",
    "            \"mc_input_ids\": mc_encoding[\"input_ids\"],\n",
    "            \"mc_attention_mask\": mc_encoding[\"attention_mask\"],\n",
    "            \"da_input_ids\": da_encoding[\"input_ids\"],\n",
    "            \"da_attention_mask\": da_encoding[\"attention_mask\"],\n",
    "            \"direct_answer_labels\": da_labels.squeeze(),\n",
    "            \"multiple_choice_label\": mc_label\n",
    "        }\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path/to/augmented_dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LR = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dir = os.getenv('COCO_DIR')\n",
    "aokvqa_dir = os.getenv('AOKVQA_DIR')\n",
    "val_dataset = load_aokvqa(aokvqa_dir, 'val')\n",
    "train_dataset = load_aokvqa(aokvqa_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = load_dataset(\"json\", data_files=\"Data/train.jsonl\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"json\", data_files=\"Data/train.jsonl\", split=\"val\")\n",
    "print(\"Training sets: {} - Validating set: {}\".format(len(training_dataset), len(valid_dataset)))\n",
    "\n",
    "train_dataset = VQADataset(dataset=training_dataset,\n",
    "                          processor=processor,\n",
    "                          coco_dir=coco_dir)\n",
    "valid_dataset = VQADataset(dataset=valid_dataset,\n",
    "                          processor=processor,\n",
    "                          coco_dir=coco_dir)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': 'train',\n",
       " 'image_id': 299207,\n",
       " 'question_id': '22MexNkBPpdZGX6sxbxVBH',\n",
       " 'question': 'What is the man by the bags awaiting?',\n",
       " 'choices': ['skateboarder', 'train', 'delivery', 'cab'],\n",
       " 'correct_choice_idx': 3,\n",
       " 'direct_answers': ['ride',\n",
       "  'ride',\n",
       "  'bus',\n",
       "  'taxi',\n",
       "  'travelling',\n",
       "  'traffic',\n",
       "  'taxi',\n",
       "  'cab',\n",
       "  'cab',\n",
       "  'his ride'],\n",
       " 'difficult_direct_answer': False,\n",
       " 'rationales': ['A train would not be on the street, he would not have luggage waiting for a delivery, and the skateboarder is there and not paying attention to him so a cab is the only possible answer.',\n",
       "  'He has bags as if he is going someone, and he is on a road waiting for vehicle that can only be moved on the road and is big enough to hold the bags.',\n",
       "  'He looks to be waiting for a paid ride to pick him up.']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CustomLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in NUM_EPOCHS:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\"):\n",
    "        # Move data to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "        mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "        da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "        da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "        mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "        da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "        # Forward pass for MC\n",
    "        mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "        mc_logits = mc_outputs.logits\n",
    "\n",
    "        # Forward pass for DA\n",
    "        da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "        da_logits = da_outputs.logits\n",
    "        \n",
    "        loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss = {total_loss / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_dataloader, desc=\"Validation\"):\n",
    "        # Move data to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "        mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "        da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "        da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "        mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "        da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "        # Forward pass for MC\n",
    "        mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "        mc_logits = mc_outputs.logits\n",
    "\n",
    "        # Forward pass for DA\n",
    "        da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "        da_logits = da_outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "avg_val_loss = val_loss / len(valid_dataloader)\n",
    "print(f\"Validation Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaprompts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
