{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingqia2/miniconda3/envs/aokvqa/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['COCO_DIR'] = '/usr1/data/mingqia2/datasets/coco/'\n",
    "os.environ['AOKVQA_DIR'] = '/usr1/data/mingqia2/aokvqa/'\n",
    "os.environ['HF_HOME'] = '/usr1/data/models_cache'\n",
    "\n",
    "import json \n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n",
    "\n",
    "from load_aokvqa import load_aokvqa, get_coco_path\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset, processor, coco_dir, max_length=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: List of samples with original question, answer, and visual clues.\n",
    "            processor: BLIP processor for text and image preprocessing.\n",
    "            image_dir: Path to the directory containing images.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.coco_dir = coco_dir\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        image_path = get_coco_path(sample['split'], sample['image_id'], self.coco_dir)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        question = sample['question']\n",
    "        visual_clues = sample.get('viper_gpt', {}).get('viper_question', '') + ' ' + sample.get('viper_gpt', {}).get('viper_response', '')\n",
    "        rationales = sample.get('rationales', [])\n",
    "        \n",
    "        # Format choices into A:xx, B:xx, ... \n",
    "        formatted_choices = ', '.join([f\"{chr(65 + i)}: {choice}\" for i, choice in enumerate(sample['choices'])])\n",
    "        correct_choice = chr(65 + sample[\"correct_choice_idx\"]) # Convert index to letter\n",
    "        direct_answers = sample['direct_answers']\n",
    "        most_frequent_answer = Counter(direct_answers).most_common()[0][0]\n",
    "\n",
    "        # Prompts for MC and DA tasks\n",
    "        mc_question = (\n",
    "            f\"Question: {question} \\n Visual Clues: {visual_clues} \\n Choices: {formatted_choices}. \"\n",
    "            \"Please provide a rationale and then return the letter of the correct answer in the format: \"\n",
    "            \"'Rationale: [your explanation] \\\\n Answer: [your answer]'.\"\n",
    "        )\n",
    "        da_question = (\n",
    "            f\"Question: {question} \\n Visual Clues: {visual_clues}. \"\n",
    "            \"Please provide a rationale and then return the direct answer in the format: \"\n",
    "            \"'Rationale: [your explanation] \\\\n Answer: [your answer]'.\"\n",
    "        )\n",
    "        \n",
    "        rationale_text = \" \".join(rationales)\n",
    "        mc_output = f\"Rationale: {rationale_text} \\n Answer: {correct_choice}\"\n",
    "        da_output = f\"Rationale: {rationale_text} \\n Answer: {most_frequent_answer}\"\n",
    "        # print(\"MC Output:\", mc_output)\n",
    "        # print(\"DA Output:\", da_output)\n",
    "        \n",
    "        # Process image and augmented text\n",
    "        mc_encoding = self.processor(image, mc_question, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        da_encoding = self.processor(image, da_question, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")  \n",
    "        \n",
    "        mc_labels = self.processor.tokenizer(\n",
    "        mc_output, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "    ).input_ids.squeeze(0)\n",
    "        # print(f\"MC Labels Shape: {mc_labels.shape}\")\n",
    "        \n",
    "        da_labels = self.processor.tokenizer(\n",
    "        da_output, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "    ).input_ids.squeeze(0)\n",
    "        # print(f\"DA Labels Shape: {da_labels.shape}\")\n",
    "    \n",
    "        # Truncate DA Input IDs to match max_length\n",
    "        # da_input_ids = da_encoding[\"input_ids\"].squeeze()\n",
    "        # if da_input_ids.size(0) > self.max_length:\n",
    "        #     da_input_ids = da_input_ids[:self.max_length]\n",
    "        # # Truncate DA Attention Mask\n",
    "        # da_attention_mask = da_encoding[\"attention_mask\"].squeeze()\n",
    "        # if da_attention_mask.size(0) > self.max_length:\n",
    "        #     da_attention_mask = da_attention_mask[:self.max_length]\n",
    "        \n",
    "        # Ensure no unexpected dimensions in labels\n",
    "        assert mc_labels.dim() == 1, f\"Unexpected MC Labels Shape: {mc_labels.shape}\"\n",
    "        assert da_labels.dim() == 1, f\"Unexpected DA Labels Shape: {da_labels.shape}\"\n",
    "           \n",
    "        return {\n",
    "            \"pixel_values\": mc_encoding[\"pixel_values\"].squeeze(0),\n",
    "            \"mc_input_ids\": mc_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"mc_attention_mask\": mc_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"mc_labels\": mc_labels,\n",
    "            # \"da_input_ids\": da_input_ids,\n",
    "            # \"da_attention_mask\": da_attention_mask,\n",
    "            \"da_input_ids\": da_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"da_attention_mask\": da_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"da_labels\": da_labels,\n",
    "        }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    mc_input_ids = torch.stack([item[\"mc_input_ids\"] for item in batch])\n",
    "    mc_attention_mask = torch.stack([item[\"mc_attention_mask\"] for item in batch])\n",
    "    mc_labels = torch.stack([item[\"mc_labels\"] for item in batch])\n",
    "    da_input_ids = torch.stack([item[\"da_input_ids\"] for item in batch])\n",
    "    da_attention_mask = torch.stack([item[\"da_attention_mask\"] for item in batch])\n",
    "    da_labels = torch.stack([item[\"da_labels\"] for item in batch])\n",
    "    \n",
    "    # print(\"Shapes after collation: \")\n",
    "    # print(f\"Pixel Values Shape: {pixel_values.shape}\")\n",
    "    # print(f\"MC Input IDs Shape: {mc_input_ids.shape}\")\n",
    "    # print(f\"MC Attention Mask Shape: {mc_attention_mask.shape}\")\n",
    "    # print(f\"MC Labels Shape: {mc_labels.shape}\")\n",
    "    # print(f\"DA Input IDs Shape: {da_input_ids.shape}\")\n",
    "    # print(f\"DA Attention Mask Shape: {da_attention_mask.shape}\")\n",
    "    # print(f\"DA Labels Shape: {da_labels.shape}\")\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"mc_input_ids\": mc_input_ids,\n",
    "        \"mc_attention_mask\": mc_attention_mask,\n",
    "        \"mc_labels\": mc_labels,\n",
    "        \"da_input_ids\": da_input_ids,\n",
    "        \"da_attention_mask\": da_attention_mask,\n",
    "        \"da_labels\": da_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, da_weight=1.0, mc_weight=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.da_weight = da_weight\n",
    "        self.mc_weight = mc_weight\n",
    "        self.loss_fn = nn.CrossEntropyLoss() \n",
    "        # self.mc_loss = nn.CrossEntropyLoss()  # Loss for MC task\n",
    "        # self.da_loss = nn.CrossEntropyLoss()  # Loss for DA task\n",
    "    \n",
    "    def forward(self, mc_logits, da_logits, mc_labels, da_labels):\n",
    "\n",
    "        # print(f\"Original MC Logits Shape: {mc_logits.shape}\")\n",
    "        # print(f\"Original MC Labels Shape: {mc_labels.shape}\")\n",
    "        # print(f\"Original DA Logits Shape: {da_logits.shape}\")\n",
    "        # print(f\"Original DA Labels Shape: {da_labels.shape}\")\n",
    "    \n",
    "        mc_logits = mc_logits.contiguous().view(-1, mc_logits.size(-1))  # Shape: [batch_size * seq_len, vocab_size]\n",
    "        mc_labels = mc_labels.contiguous().view(-1)                   # Shape: [batch_size * seq_len]\n",
    "\n",
    "        da_logits = da_logits.contiguous().view(-1, da_logits.size(-1))  # Shape: [batch_size * seq_len, vocab_size]\n",
    "        da_labels = da_labels.contiguous().view(-1)                    # Shape: [batch_size * seq_len]\n",
    "        \n",
    "        # print(f\"Flattened MC Logits Shape: {mc_logits.shape}\")\n",
    "        # print(f\"Flattened MC Labels Shape: {mc_labels.shape}\")\n",
    "        # print(f\"Flattened DA Logits Shape: {da_logits.shape}\")\n",
    "        # print(f\"Flattened DA Labels Shape: {da_labels.shape}\")\n",
    "\n",
    "        mc_loss = self.loss_fn(mc_logits, mc_labels)\n",
    "        da_loss = self.loss_fn(da_logits, da_labels)\n",
    "        # total_loss = mc_loss + da_loss\n",
    "        total_loss = self.mc_weight * mc_loss + self.da_weight * da_loss\n",
    "        return total_loss, mc_loss, da_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, processor, valid_dataloader, epochs, learning_rate, device, da_weight=1.0, mc_weight=1.0):\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        loss_fn = CustomLoss(da_weight=da_weight, mc_weight=mc_weight)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, total_mc_loss, total_da_loss = 0, 0, 0\n",
    "            \n",
    "            for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                print(f\"Pixel Values Shape: {pixel_values.shape}\")\n",
    "                \n",
    "                # MC task\n",
    "                mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "                mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "                mc_labels = batch[\"mc_labels\"].to(device)\n",
    "\n",
    "                mc_outputs = model(\n",
    "                    pixel_values=pixel_values, \n",
    "                    input_ids=mc_input_ids, \n",
    "                    attention_mask=mc_attention_mask, \n",
    "                    # labels=mc_labels\n",
    "                )\n",
    "\n",
    "                # DA task\n",
    "                da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "                da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "                da_labels = batch[\"da_labels\"].to(device)\n",
    "\n",
    "                # print(f\"DA Input IDs Shape: {da_input_ids.shape}\")\n",
    "                # print(f\"DA Attention Mask Shape: {da_attention_mask.shape}\")\n",
    "                # print(f\"DA Labels Shape: {da_labels.shape}\")\n",
    "\n",
    "                da_outputs = model(\n",
    "                    pixel_values=pixel_values, \n",
    "                    input_ids=da_input_ids, \n",
    "                    attention_mask=da_attention_mask, \n",
    "                    # labels=da_labels\n",
    "                )\n",
    "                # print(f\"DA Logits Shape: {da_outputs.logits.shape}\")\n",
    "                \n",
    "                mc_logits = mc_outputs.logits[:, :mc_labels.size(1), :]  # Shape: [batch_size, label_seq_len, vocab_size]\n",
    "                da_logits = da_outputs.logits[:, :da_labels.size(1), :]  # Shape: [batch_size, label_seq_len, vocab_size]\n",
    "                # Compute weighted losses\n",
    "                loss, mc_loss, da_loss = loss_fn(mc_logits, da_logits, mc_labels, da_labels)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_mc_loss += mc_loss.item()\n",
    "                total_da_loss += da_loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: Total Loss = {total_loss / len(train_dataloader):.4f}, \"\n",
    "                f\"MC Loss = {total_mc_loss / len(train_dataloader):.4f}, \"\n",
    "                f\"DA Loss = {total_da_loss / len(train_dataloader):.4f}\")\n",
    "            \n",
    "            # Evaluate after each epoch\n",
    "            # evaluate_model(model, processor, valid_dataloader, device, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 11271 samples, Validation set: 774 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/1409 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values Shape: torch.Size([8, 3, 224, 224])\n",
      "Original MC Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original MC Labels Shape: torch.Size([8, 256])\n",
      "Original DA Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original DA Labels Shape: torch.Size([8, 256])\n",
      "Flattened MC Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened MC Labels Shape: torch.Size([2048])\n",
      "Flattened DA Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened DA Labels Shape: torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 1/1409 [00:04<1:38:07,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values Shape: torch.Size([8, 3, 224, 224])\n",
      "Original MC Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original MC Labels Shape: torch.Size([8, 256])\n",
      "Original DA Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original DA Labels Shape: torch.Size([8, 256])\n",
      "Flattened MC Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened MC Labels Shape: torch.Size([2048])\n",
      "Flattened DA Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened DA Labels Shape: torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 2/1409 [00:06<1:16:41,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values Shape: torch.Size([8, 3, 224, 224])\n",
      "Original MC Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original MC Labels Shape: torch.Size([8, 256])\n",
      "Original DA Logits Shape: torch.Size([8, 256, 50304])\n",
      "Original DA Labels Shape: torch.Size([8, 256])\n",
      "Flattened MC Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened MC Labels Shape: torch.Size([2048])\n",
      "Flattened DA Logits Shape: torch.Size([2048, 50304])\n",
      "Flattened DA Labels Shape: torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 2/1409 [00:09<1:46:58,  4.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mvqa_collate_fn)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mda_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDA_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmc_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMC_WEIGHT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, processor, valid_dataloader, epochs, learning_rate, device, da_weight, mc_weight)\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     53\u001b[0m total_mc_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mc_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/optim/adamw.py:145\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 145\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/optim/_functional.py:151\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    149\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    153\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    155\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LR = 5e-4\n",
    "DA_WEIGHT = 1.0\n",
    "MC_WEIGHT = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n",
    "\n",
    "coco_dir = os.getenv('COCO_DIR')\n",
    "aokvqa_dir = os.getenv('AOKVQA_DIR')\n",
    "\n",
    "train_path = \"../results/viper_augmentations/aokvqa_plus_viper_train.json\"\n",
    "val_path = \"../results/viper_augmentations/aokvqa_plus_viper_val.json\"\n",
    "\n",
    "training_dataset = load_dataset(\"json\", data_files={\"train\": train_path}, split=\"train\")\n",
    "valid_dataset = load_dataset(\"json\", data_files={\"val\": val_path}, split=\"val\")\n",
    "\n",
    "print(f\"Training set: {len(training_dataset)} samples, Validation set: {len(valid_dataset)} samples\")\n",
    "\n",
    "train_dataset = VQADataset(dataset=training_dataset, processor=processor, coco_dir=coco_dir)\n",
    "valid_dataset = VQADataset(dataset=valid_dataset, processor=processor, coco_dir=coco_dir)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=vqa_collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=vqa_collate_fn)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    processor=processor,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    device=device,\n",
    "    da_weight=DA_WEIGHT,\n",
    "    mc_weight=MC_WEIGHT,\n",
    "    )\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./trained_model\")\n",
    "processor.save_pretrained(\"./trained_model\")\n",
    "print(\"Model and processor saved to './trained_model'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mc_loss = 0\n",
    "    total_da_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "            # MC task\n",
    "            mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "            mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "            mc_labels = batch[\"mc_labels\"].to(device)\n",
    "\n",
    "            mc_outputs = model(\n",
    "                pixel_values=pixel_values, \n",
    "                input_ids=mc_input_ids, \n",
    "                attention_mask=mc_attention_mask, \n",
    "                labels=mc_labels\n",
    "            )\n",
    "\n",
    "            # DA task\n",
    "            da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "            da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "            da_labels = batch[\"da_labels\"].to(device)\n",
    "\n",
    "            da_outputs = model(\n",
    "                pixel_values=pixel_values, \n",
    "                input_ids=da_input_ids, \n",
    "                attention_mask=da_attention_mask, \n",
    "                labels=da_labels\n",
    "            )\n",
    "\n",
    "            # Compute weighted losses\n",
    "            loss, mc_loss, da_loss = loss_fn(mc_outputs.logits, mc_labels, da_outputs.logits, da_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mc_loss += mc_loss.item()\n",
    "            total_da_loss += da_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_mc_loss = total_mc_loss / len(dataloader)\n",
    "    avg_da_loss = total_da_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Validation: Total Loss = {avg_loss:.4f}, MC Loss = {avg_mc_loss:.4f}, DA Loss = {avg_da_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 3/1409 [00:08<1:10:16,  3.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(mc_logits\u001b[38;5;241m=\u001b[39mmc_logits, da_logits\u001b[38;5;241m=\u001b[39mda_logits, mc_labels\u001b[38;5;241m=\u001b[39mmc_labels, da_labels\u001b[38;5;241m=\u001b[39mda_labels)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NUM_EPOCHS = 3\n",
    "# LR = 5e-4\n",
    "\n",
    "# loss_fn = CustomLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\"):\n",
    "#         # Move data to device\n",
    "#         pixel_values = batch[\"pixel_values\"].to(device)\n",
    "#         mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "#         mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "#         da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "#         da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "#         mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "#         da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "#         # Forward pass for MC\n",
    "#         mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "#         mc_logits = mc_outputs.logits\n",
    "\n",
    "#         # Forward pass for DA\n",
    "#         da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "#         da_logits = da_outputs.logits\n",
    "        \n",
    "#         # print(f\"mc_logits shape: {mc_logits.shape}, mc_labels shape: {mc_labels.shape}\")\n",
    "#         # print(f\"da_logits shape: {da_logits.shape}\") \n",
    "#         # print(f\"da_labels shape: {da_labels.shape}\")\n",
    "        \n",
    "#         loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f\"Epoch {epoch}: Loss = {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# val_loss = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(valid_dataloader, desc=\"Validation\"):\n",
    "#         # Move data to device\n",
    "#         pixel_values = batch[\"pixel_values\"].to(device)\n",
    "#         mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "#         mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "#         da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "#         da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "#         mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "#         da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "#         # Forward pass for MC\n",
    "#         mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "#         mc_logits = mc_outputs.logits\n",
    "\n",
    "#         # Forward pass for DA\n",
    "#         da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "#         da_logits = da_outputs.logits\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "#         val_loss += loss.item()\n",
    "\n",
    "# avg_val_loss = val_loss / len(valid_dataloader)\n",
    "# print(f\"Validation Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aokvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
