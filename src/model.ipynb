{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingqia2/miniconda3/envs/aokvqa/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['COCO_DIR'] = '/usr1/data/mingqia2/datasets/coco/'\n",
    "os.environ['AOKVQA_DIR'] = '/usr1/data/mingqia2/aokvqa/'\n",
    "os.environ['HF_HOME'] = '/usr1/data/models_cache'\n",
    "\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n",
    "\n",
    "from load_aokvqa import load_aokvqa, get_coco_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, dataset, processor, coco_dir, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: List of samples with original question, answer, and visual clues.\n",
    "            processor: BLIP processor for text and image preprocessing.\n",
    "            image_dir: Path to the directory containing images.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.coco_dir = coco_dir\n",
    "        self.max_length = max_length\n",
    "        # self.split = split # train or val \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "         # Get image path and open image\n",
    "        image_path = get_coco_path(sample['split'], sample['image_id'], self.coco_dir)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Combine question with MC options\n",
    "        question = sample['question']\n",
    "        choices = sample['choices']\n",
    "        alphabet = ['A', 'B', 'C', 'D']\n",
    "        # Format the choices\n",
    "        formatted_choices = ', '.join([f\"{letter}: {choice}\" for letter, choice in zip(alphabet, choices)])\n",
    "        \n",
    "        \n",
    "        # Get visual clues \n",
    "        visual_clues = sample.get('viper_gpt', {}).get('viper_question', '') + ' ' + sample.get('viper_gpt', {}).get('viper_response', '')\n",
    "\n",
    "        mc_question = f\"Question: {question} Visual Clues: {visual_clues} Choices: {formatted_choices}. Please only return the letter of the correct answer.\"\n",
    "        da_question = f\"Question: {question} Visual Clues: {visual_clues}\"\n",
    "        \n",
    "        # Process image and augmented text\n",
    "        mc_encoding = self.processor(image, mc_question, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        da_encoding = self.processor(image, da_question, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Prepare answers \n",
    "        direct_answers = sample['direct_answers']\n",
    "        correct_choice_idx = sample['correct_choice_idx'] # index of the correct MC choice\n",
    "        \n",
    "        da_text = \" | \".join(direct_answers)\n",
    "        da_labels = self.processor.tokenizer(\n",
    "            da_text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "        mc_label = torch.tensor(correct_choice_idx, dtype=torch.long)\n",
    "        \n",
    "        encoding = {\n",
    "            \"pixel_values\": mc_encoding[\"pixel_values\"].squeeze(0),  \n",
    "            \"mc_input_ids\": mc_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"mc_attention_mask\": mc_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"da_input_ids\": da_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"da_attention_mask\": da_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"direct_answer_labels\": da_labels,\n",
    "            \"multiple_choice_label\": mc_label\n",
    "        }\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mc_loss = nn.CrossEntropyLoss()  # Loss for MC task\n",
    "        self.da_loss = nn.CrossEntropyLoss()  # Loss for DA task\n",
    "        # self.ce_loss = nn.CrossEntropyLoss() # overall loss  \n",
    "        # self.aux_loss = nn.MSELoss()  # Auxiliary loss for alignment\n",
    "    \n",
    "    def forward(self, mc_logits, da_logits, mc_labels, da_labels):\n",
    "        mc_logits = mc_logits[:, 0, :]\n",
    "\n",
    "        min_seq_length = min(da_logits.size(1), da_labels.size(1))\n",
    "        da_logits = da_logits[:, :min_seq_length, :].contiguous()\n",
    "        da_labels = da_labels[:, :min_seq_length].contiguous()\n",
    "        \n",
    "        # Flatten da_logits and da_labels for loss computation\n",
    "        da_logits = da_logits.view(-1, da_logits.size(-1)) # [batch_size * seq_length, vocab_size]\n",
    "        da_labels = da_labels.view(-1) # [batch_size * seq_length]                    \n",
    "        \n",
    "        mc_loss = self.mc_loss(mc_logits, mc_labels)\n",
    "        da_loss = self.da_loss(da_logits, da_labels)\n",
    "        total_loss = mc_loss + da_loss\n",
    "        return total_loss \n",
    "    \n",
    "        # Auxiliary loss to align metadata and question embeddings\n",
    "        # aux = self.aux_loss(metadata_embeddings, question_embeddings)\n",
    "        # return ce + 0.1 * aux  # Combine with weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 11271 samples, Validation set: 774 samples\n"
     ]
    }
   ],
   "source": [
    "coco_dir = os.getenv('COCO_DIR')\n",
    "aokvqa_dir = os.getenv('AOKVQA_DIR')\n",
    "\n",
    "train_path = \"../results/viper_augmentations/aokvqa_plus_viper_train.json\"\n",
    "val_path = \"../results/viper_augmentations/aokvqa_plus_viper_val.json\"\n",
    "\n",
    "training_dataset = load_dataset(\"json\", data_files={\"train\": train_path}, split=\"train\")\n",
    "valid_dataset = load_dataset(\"json\", data_files={\"val\": val_path}, split=\"val\")\n",
    "\n",
    "print(f\"Training set: {len(training_dataset)} samples, Validation set: {len(valid_dataset)} samples\")\n",
    "\n",
    "train_dataset = VQADataset(dataset=training_dataset, processor=processor, coco_dir=coco_dir)\n",
    "valid_dataset = VQADataset(dataset=valid_dataset, processor=processor, coco_dir=coco_dir)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 1.5070,  1.5216,  1.5800,  ..., -1.4711, -1.3835,  0.2807],\n",
       "          [ 1.5654,  1.5800,  1.5800,  ..., -1.6463, -0.1280,  0.6165],\n",
       "          [ 1.6530,  1.6676,  1.6238,  ..., -1.4565, -0.5368,  0.3975],\n",
       "          ...,\n",
       "          [ 0.2515,  0.3245, -0.6536,  ...,  0.4559,  0.3245,  0.4267],\n",
       "          [ 0.0909,  0.2515, -0.5222,  ...,  0.5289,  0.3099,  0.3245],\n",
       "          [ 0.1347,  0.3245, -0.4930,  ...,  0.3829,  0.3829,  0.2953]],\n",
       " \n",
       "         [[ 1.8047,  1.8047,  1.8498,  ..., -1.3169, -1.2118,  0.9493],\n",
       "          [ 1.8198,  1.8498,  1.8648,  ..., -1.5420,  0.3490,  1.3995],\n",
       "          [ 1.9098,  1.8948,  1.8798,  ..., -1.3469, -0.0712,  1.1444],\n",
       "          ...,\n",
       "          [ 0.2589,  0.4090, -0.5815,  ...,  0.4691,  0.3790,  0.4240],\n",
       "          [ 0.1089,  0.3190, -0.4464,  ...,  0.5591,  0.3490,  0.3340],\n",
       "          [ 0.1989,  0.3940, -0.3864,  ...,  0.4090,  0.4240,  0.3190]],\n",
       " \n",
       "         [[ 2.0890,  2.1032,  2.1032,  ..., -1.0678, -0.9399,  1.5202],\n",
       "          [ 2.1032,  2.0748,  2.1032,  ..., -1.3949,  0.6528,  2.1317],\n",
       "          [ 2.1317,  2.1175,  2.1317,  ..., -0.9114,  0.0982,  1.5202],\n",
       "          ...,\n",
       "          [ 0.4821,  0.4964, -0.5701,  ...,  0.6101,  0.4964,  0.5390],\n",
       "          [ 0.3399,  0.4110, -0.4137,  ...,  0.6670,  0.4679,  0.4537],\n",
       "          [ 0.3826,  0.6101, -0.3284,  ...,  0.5106,  0.5390,  0.4395]]]),\n",
       " 'mc_input_ids': tensor([50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265,     2, 45641,    35,   653,    16,     5,   313,    30,\n",
       "             5,  5565, 10254,   116, 25878,  2893,  3663,    35, 27705, 21700,\n",
       "             5,   511, 44875,    29,     8,   141,    49,  4158,    50, 11324,\n",
       "            11,   112,    12,   176, 11305,    35,   313,     6,  5565,    20,\n",
       "           313,     8,  5565,    32, 35642,    11,     5,  2274,     4, 11501,\n",
       "          6355,    35,    83,    35, 12800,  4929,   254,     6,   163,    35,\n",
       "          2341,     6,   230,    35,  2996,     6,   211,    35, 11148,     4,\n",
       "          3401,   129,   671,     5,  1601,     9,     5,  4577,  1948,     4,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'mc_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'da_input_ids': tensor([50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265,\n",
       "         50265, 50265,     2, 45641,    35,   653,    16,     5,   313,    30,\n",
       "             5,  5565, 10254,   116, 25878,  2893,  3663,    35, 27705, 21700,\n",
       "             5,   511, 44875,    29,     8,   141,    49,  4158,    50, 11324,\n",
       "            11,   112,    12,   176, 11305,    35,   313,     6,  5565,    20,\n",
       "           313,     8,  5565,    32, 35642,    11,     5,  2274,     4,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'da_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'direct_answer_labels': tensor([    2, 23167,  1721,  3068,  1721,  2353,  1721,  9955,  1721,  7290,\n",
       "          1721,  1703,  1721,  9955,  1721, 11148,  1721, 11148,  1721,    39,\n",
       "          3068,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'multiple_choice_label': tensor(3)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 3/1409 [00:08<1:10:16,  3.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(mc_logits\u001b[38;5;241m=\u001b[39mmc_logits, da_logits\u001b[38;5;241m=\u001b[39mda_logits, mc_labels\u001b[38;5;241m=\u001b[39mmc_labels, da_labels\u001b[38;5;241m=\u001b[39mda_labels)\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aokvqa/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LR = 5e-4\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\"):\n",
    "        # Move data to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "        mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "        da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "        da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "        mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "        da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "        # Forward pass for MC\n",
    "        mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "        mc_logits = mc_outputs.logits\n",
    "\n",
    "        # Forward pass for DA\n",
    "        da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "        da_logits = da_outputs.logits\n",
    "        \n",
    "        # print(f\"mc_logits shape: {mc_logits.shape}, mc_labels shape: {mc_labels.shape}\")\n",
    "        # print(f\"da_logits shape: {da_logits.shape}\") \n",
    "        # print(f\"da_labels shape: {da_labels.shape}\")\n",
    "        \n",
    "        loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss = {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_dataloader, desc=\"Validation\"):\n",
    "        # Move data to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        mc_input_ids = batch[\"mc_input_ids\"].to(device)\n",
    "        mc_attention_mask = batch[\"mc_attention_mask\"].to(device)\n",
    "        da_input_ids = batch[\"da_input_ids\"].to(device)\n",
    "        da_attention_mask = batch[\"da_attention_mask\"].to(device)\n",
    "        mc_labels = batch[\"multiple_choice_label\"].to(device)\n",
    "        da_labels = batch[\"direct_answer_labels\"].to(device)\n",
    "\n",
    "        # Forward pass for MC\n",
    "        mc_outputs = model(pixel_values=pixel_values, input_ids=mc_input_ids, attention_mask=mc_attention_mask)\n",
    "        mc_logits = mc_outputs.logits\n",
    "\n",
    "        # Forward pass for DA\n",
    "        da_outputs = model(pixel_values=pixel_values, input_ids=da_input_ids, attention_mask=da_attention_mask)\n",
    "        da_logits = da_outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(mc_logits=mc_logits, da_logits=da_logits, mc_labels=mc_labels, da_labels=da_labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "avg_val_loss = val_loss / len(valid_dataloader)\n",
    "print(f\"Validation Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aokvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
