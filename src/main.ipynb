{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process any remaining queries that did not fill a complete batch\n",
    "# if queries:\n",
    "#     out = pipe(queries, max_length=100, num_return_sequences=1, temperature=0.01)\n",
    "#     for i, pred in enumerate(out):\n",
    "#         # Since `pred` is a list of dictionaries, access the first element to get the generated text\n",
    "#         generated_text = pred[0]['generated_text'] if isinstance(pred, list) and len(pred) > 0 else \"\"\n",
    "        \n",
    "#         # Strip out the prompt and process each prediction\n",
    "#         stripped_pred = generated_text[len(queries[i]):].strip().lower()\n",
    "#         stripped_pred = stripped_pred.split('\\n')[0].strip()\n",
    "#         stripped_pred = stripped_pred.rstrip(string.punctuation)\n",
    "\n",
    "#         # Store the result\n",
    "#         results[question_ids[i]][mode] = stripped_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmire/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting split: validation\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import string\n",
    "import os\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text-generation\", model=model_name, pad_token_id=tokenizer.eos_token_id, device_map='auto')\n",
    "\n",
    "def get_query_for_mode(question, choices, mode):\n",
    "    if mode == 'multiple_choice':\n",
    "        return f\"{question} Choose one of the following: {', '.join(choices)}. Don't include any other text in your response.\"\n",
    "    else:\n",
    "        return f\"{question} Respond in as few words as possible.\"\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceM4/A-OKVQA\")\n",
    "modes = ['multiple_choice', 'direct_answer']\n",
    "splits = ['validation']\n",
    "results_dir = 'results'\n",
    "\n",
    "for split in splits:\n",
    "    print(f'Starting split: {split}')\n",
    "    results_path = f'../{results_dir}/{split}.json'\n",
    "    if os.path.exists(results_path):\n",
    "        continue\n",
    "    results = defaultdict(dict)\n",
    "    for example in tqdm(ds[split]):\n",
    "        input_question = example['question']\n",
    "        question_id = example['question_id']\n",
    "        \n",
    "        for mode in modes:\n",
    "            query = get_query_for_mode(input_question, example.get('choices', []), mode)\n",
    "            out = pipe(query, max_length=100, num_return_sequences=1, temperature=0.01)  # Use a small positive temperature\n",
    "            \n",
    "            # Get generated text\n",
    "            pred = out[0]['generated_text']\n",
    "            \n",
    "            # Strip out the prompt\n",
    "            stripped_pred = pred[len(query):].strip().lower()\n",
    "            stripped_pred = stripped_pred.split('\\n')[0].strip()\n",
    "            stripped_pred = stripped_pred.rstrip(string.punctuation)\n",
    "            \n",
    "            # print('-----------')\n",
    "            # print(query)\n",
    "            # print(stripped_pred)\n",
    "            # Store the result\n",
    "            results[question_id][mode] = stripped_pred\n",
    "\n",
    "    print(f'Saving results for split: {split}')\n",
    "    with open(results_path, 'w') as json_file:\n",
    "        json.dump(results, json_file, indent=4)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [00:01<00:00, 965.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 8B Instruct validation\n",
      "mc acc:  0.212\n",
      "da acc:  0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_string(s):\n",
    "    # Convert to lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    \n",
    "    # Remove articles (a, an, the)\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', '', s)\n",
    "    \n",
    "    # Convert words to digits (if they are numbers)\n",
    "    s = re.sub(r'\\bzero\\b', '0', s)\n",
    "    s = re.sub(r'\\bone\\b', '1', s)\n",
    "    s = re.sub(r'\\btwo\\b', '2', s)\n",
    "    s = re.sub(r'\\bthree\\b', '3', s)\n",
    "    s = re.sub(r'\\bfour\\b', '4', s)\n",
    "    s = re.sub(r'\\bfive\\b', '5', s)\n",
    "    s = re.sub(r'\\bsix\\b', '6', s)\n",
    "    s = re.sub(r'\\bseven\\b', '7', s)\n",
    "    s = re.sub(r'\\beight\\b', '8', s)\n",
    "    s = re.sub(r'\\bnine\\b', '9', s)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "for split in splits:\n",
    "  results_path = f'../{results_dir}/{split}.json'\n",
    "  with open(results_path, 'r') as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "  mc_correct_list = []\n",
    "  da_correct_list = []\n",
    "  for example in tqdm(ds[split]):\n",
    "    qid = example['question_id']\n",
    "    target_mc = example['choices'][example['correct_choice_idx']]\n",
    "    target_da_list = example['direct_answers']\n",
    "    pred_mc = results[qid]['multiple_choice']\n",
    "    pred_da = format_string(results[qid]['direct_answer'])\n",
    "    \n",
    "\n",
    "    mc_correct_list.append(pred_mc == target_mc)\n",
    "    da_correct_list.append(min(target_da_list.count(pred_da)/10, 1))\n",
    "\n",
    "  mc_acc = sum(mc_correct_list) / len(mc_correct_list)\n",
    "  da_acc = sum(da_correct_list) / len(da_correct_list)\n",
    "  print('Llama 3 8B Instruct', split)\n",
    "  print('mc acc: ', round(mc_acc, 3))\n",
    "  print('da acc: ', round(da_acc, 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
